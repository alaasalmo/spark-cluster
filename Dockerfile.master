FROM openjdk:11-jre-slim

# Environment variables for Spark
ENV SPARK_VERSION=3.5.4 \
    HADOOP_VERSION=3 \
    SPARK_HOME=/opt/spark \
    PATH=$SPARK_HOME/bin:$PATH
	
RUN apt-get update && apt-get install -y \
    curl \
    git \
    python3 \
    python3-pip \
    unzip \
    && rm -rf /var/lib/apt/lists/*

# Install procps to get the ps command
RUN apt-get update && apt-get install -y procps && apt-get clean

# Example command to check if ps works
RUN ps -e

RUN apt update && apt-get install net-tools
RUN apt-get clean && apt-get update && apt-get install -y netcat

# Install required packages
RUN apt-get update && apt-get install -y --no-install-recommends bash curl && \
    curl -sL https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz | tar xvz -C /opt && \
    mv /opt/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /opt/spark && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# Expose ports
EXPOSE 7077 8080
COPY start-spark.sh .

# Command to start Spark master
ENTRYPOINT ["bash", "-c", "./start-spark.sh"]
#CMD ["bash", "-c", "$SPARK_HOME/sbin/start-master.sh && tail -f $SPARK_HOME/logs/*"]
#CMD ["sh", "-c", "tail -f /dev/null"]
