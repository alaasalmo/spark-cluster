docker build -t spark-docker .
docker build -t spark-master:0.1.0 -f Dockerfile.master .
docker build -t spark-worker:0.1.0 -f Dockerfile.worker .


docker images
docker tag 06f728016256 alaasalmo/spark-master:1.0
docker push alaasalmo/spark-master:1.0

docker images
docker tag 976adcbe0f5d alaasalmo/spark-worker:1.0
docker push alaasalmo/spark-worker:1.0




Deploymanet on minikube

kubectl apply -f spark-master-deployment.yaml
kubectl expose deployment spark-master --type=ClusterIP --name=spark-master-service --port=7077
kubectl expose deployment spark-master --type=NodePort --name=spark-master-service-web --port=8080


kubectl apply -f spark-worker-deployment.yaml
kubectl expose deployment spark-worker --type=NodePort --name=spark-worker-service --port=8081



nc spark-worker 8081

Get web:
kubectl get services

$ kubectl get services
NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes                 ClusterIP   10.96.0.1      <none>        443/TCP          69m
spark-master-service       ClusterIP   10.96.17.171   <none>        7077/TCP         3m34s
spark-master-service-web   NodePort    10.99.152.96   <none>        8080:30060/TCP   3m18s
spark-worker-service       NodePort    10.107.26.50   <none>        8081:32318/TCP   2m32s


curl http://spark-master-service:7077

minikube service spark-master-service-web

$ minikube service spark-master-service-web
|-----------|--------------------------|-------------|-----------------------------|
| NAMESPACE |           NAME           | TARGET PORT |             URL             |
|-----------|--------------------------|-------------|-----------------------------|
| default   | spark-master-service-web |        8080 | http://172.21.123.153:30060 |
|-----------|--------------------------|-------------|-----------------------------|
* Opening service default/spark-master-service-web in default browser...


kubectl port-forward svc/spark-master-service-web 8080:8080

Test for docker:

docker run -d --name spark-master -p 8080:8080 -p 7077:7077 spark-master
docker run -d --name spark-worker-1 --link spark-master:spark-master -p 8081:8081 spark-worker:1.0
docker run -d --name spark-worker-2 --link spark-master:spark-master -p 8082:8081 spark-worker:1.0


Build the services:

kubectl expose deployment spark-master --type=ClusterIP --name=spark-master-service --port=7077
kubectl expose deployment spark-master --type=NodePort --name=spark-master-service-web --port=8080
kubectl expose deployment spark-worker --type=NodePort --name=spark-worker-service --port=8081


docker build -t spark-edgenode:1.0 -f Dockerfile.edgenode .
kubectl apply -f spark-edge-node-deployment.yaml
kubectl exec -it <edge-node-pod-name> -n spark -- /bin/bash


kubectl exec -it deployment-edgenode-df88bcd49-rrbct -- /bin/bash

spark-submit \
  --master k8s://https://$(minikube ip):8443 \
  --deploy-mode cluster \
  --name spark-pi \
  --class org.apache.spark.examples.SparkPi \
  --conf spark.kubernetes.container.image=bitnami/spark:3.5.0 \
  local:///opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar 100


Python spark:


from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession.builder.appName("WordCount").getOrCreate()

# Read the input text file
input_path = "file:///opt/spark-data/sample.txt"  # Adjust path if needed
text_rdd = spark.sparkContext.textFile(input_path)

# Perform word count
word_counts = text_rdd.flatMap(lambda line: line.split(" ")) \
    .map(lambda word: (word, 1)) \
    .reduceByKey(lambda a, b: a + b)

# Save the results to an output path
output_path = "file:///opt/spark-data/output"  # Adjust path if needed
word_counts.saveAsTextFile(output_path)

# Stop the Spark session
spark.stop()


  

spark-submit \
  --master spark://spark-master-service:7077 \
  --deploy-mode client \
  --name word-count-job \
  --py-files word_count.py \
  --conf spark.executor.instances=1 \
  --conf spark.executor.memory=1g \
  --conf spark.driver.memory=1g \
  --conf spark.speculation=true \
  test.py

spark-submit  --verbose \
  --master local \
  --deploy-mode client \
  --name word-count-job \
  --py-files /test.py \
  --conf spark.executor.instances=1 \
  --conf spark.executor.memory=1g \
  --conf spark.driver.memory=1g \
  --conf spark.speculation=true


spark-submit --master spark://spark-master-service:7077 \
             --executor-memory 1G \
             --executor-cores 1 \
             --num-executors 1 \
             --conf spark.sql.shuffle.partitions=500 \
             /test.py


spark-submit --master local /test.py 
spark-submit --master spark://spark-master-service:7077 /test.py 


from pyspark.sql import SparkSession
from datetime import datetime, date
from pyspark.sql import Row

spark = SparkSession.builder.master("spark://spark-master-service:7077").appName("test").getOrCreate()

df = spark.createDataFrame([
    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),
    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),
    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))
])





spark.stop()




from pyspark.sql import SparkSession

if __name__ == "__main__":
    spark = SparkSession.builder.appName("Spark Submit Example").getOrCreate()
    data = [("Alice", 28), ("Bob", 35), ("Cathy", 45)]
    df = spark.createDataFrame(data, ["Name", "Age"])
    df.show()
    spark.stop()


spark-submit --master spark://spark-master-service:7077 --executor-memory 256 GB --executor-cores 100m --conf spark.executor.heartbeatInterval=60s --conf spark.network.timeout=120s /test.py 





from pyspark import SparkContext
from pyspark import SparkConf
from pyspark.sql import Row

sc = SparkContext(conf=conf)
RddDataSet = sc.textFile("file:///word_count.dat");
words = RddDataSet.flatMap(lambda x: x.split(" "))
result = words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)
result = result.collect()
for word in result:
    print("%s: %s" %(word[0], word[1]))



netstat -tuln | grep 4040
