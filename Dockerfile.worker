FROM openjdk:11-jre-slim

# Environment variables for Spark
ENV SPARK_VERSION=3.5.4 \
    HADOOP_VERSION=3 \
    SPARK_HOME=/opt/spark \
    PATH=$SPARK_HOME/bin:$PATH

RUN apt-get update && apt-get install -y \
    curl \
    git \
    python3 \
    python3-pip \
    unzip \
    && rm -rf /var/lib/apt/lists/*
    

# Install dependencies
RUN apt-get update && apt-get install -y procps && apt-get clean

# Example command to check if ps works
RUN ps -e

RUN apt update && apt-get install net-tools
RUN apt-get clean && apt-get update && apt-get install -y netcat

# Install required packages

RUN apt-get update && apt-get install -y --no-install-recommends bash curl && \
    curl -sL https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz | tar xvz -C /opt && \
    mv /opt/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /opt/spark && \
    apt-get clean && rm -rf /var/lib/apt/lists/*


COPY start-worker-spark.sh .
RUN chmod 755 /start-worker-spark.sh

# Expose ports
EXPOSE 8081

# Command to start Spark Worker
# CMD ["bash", "-c", "$SPARK_HOME/sbin/start-worker.sh $SPARK_MASTER_URL && tail -f $SPARK_HOME/logs/*"]
ENTRYPOINT ["bash", "-c", "./start-worker-spark.sh"]
#CMD ["sh", "-c", "tail -f /dev/null"]

